{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56358754",
   "metadata": {},
   "source": [
    "This project is a significant component of the capstone project I completed during my master’s program at UCSB. Its primary objective is to quantify uncertainty in climate model outputs and use sensitivity analysis to identify the most influential variables. The dataset encompasses a substantial volume of 4TB. During the sensitivity analysis phase, the data will be derived from a machine learning-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c53254e5-331b-4ce1-87cb-95e619e601eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load all required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxr\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# import packages and functions from utils\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xarray'"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Importing utility functions and modules\n",
    "from utils import *\n",
    "import importlib\n",
    "import utils\n",
    "\n",
    "# Importing machine learning and statistical libraries\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    cross_val_score, \n",
    "    cross_val_predict\n",
    ")\n",
    "from sklearn.gaussian_process.kernels import (\n",
    "    RationalQuadratic, \n",
    "    Sum, \n",
    "    ConstantKernel, \n",
    "    RBF, \n",
    "    Matern, \n",
    "    DotProduct\n",
    ")\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Importing helper for kernel combinations\n",
    "from itertools import combinations\n",
    "\n",
    "# Importing Matplotlib patches for custom legends\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f070ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For updates in packages, rereun the utils to store all new functions and packages\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ca9baa-2d34-4043-9865-1ff864858342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "# Request an additional 10 cores of power for processing from the server\n",
    "client = get_cluster(\"UCSB0021\", cores = 40)\n",
    "# apply peer2peer network communication across multiple devices\n",
    "client.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7cf4955-6318-4296-b528-870acc61d4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the function read_n_wrangle that utilizes functions from utils package\n",
    "def read_n_wrangle(param, var, time_selection):\n",
    "    params = param_wrangling()\n",
    "    filepath = os.path.join(\"saves\", f\"{var}.nc\")\n",
    "    global param_name, var_name\n",
    "    param_name = param\n",
    "    var_name = var\n",
    "    var_da = read_all_simulations2(var, time_selection)\n",
    "    var_avg = wrangle_var_cluster(var_da)\n",
    "    var_av = var_avg[var].values.flatten()\n",
    "    return params, var_av, param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85755299-20bf-455d-b923-238b0b418078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function for one set of values\n",
    "params, var, param = read_n_wrangle(\"KRMAX\", \"NBP\", '2005-2015')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c248bc2-31f3-4098-93cb-fd8f6956d3ac",
   "metadata": {},
   "source": [
    "Utilize all possible kernel combinations and apply a permutation-based approach to identify the optimal combination. This method involves testing all available statistical models and their combinations. While this process may run slower due to the extensive exploration of combinations, it has the potential to achieve higher R-squared values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b4ffad-b548-4c5b-85a0-b74aed2c7591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_emulator(params, var):\n",
    "    # Splitting the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        params, var, test_size=0.2, random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dec816",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Define the kernels combinations, based on literature review\n",
    "    kernels = [\n",
    "        DotProduct(sigma_0_bounds=(1e-12, 1e12)),\n",
    "        RBF(length_scale_bounds=(1e-12, 1e12)),\n",
    "        RationalQuadratic(length_scale_bounds=(1e-12, 1e12)),\n",
    "        Matern(nu=1.5, length_scale_bounds=(1e-12, 1e12)),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a1c978",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Initialize variables to store each values using list methods. \n",
    "    best_model = None\n",
    "    best_kernel_combo = None\n",
    "    best_score = float(\"-inf\")\n",
    "\n",
    "    # Generate all possible combinations of kernels\n",
    "    kernel_combinations = []\n",
    "    for r in range(1, len(kernels) + 1):\n",
    "        kernel_combinations.extend(combinations(kernels, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbebddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Train a Gaussian process regressor for each kernel combination\n",
    "    for kernel_combo in kernel_combinations:\n",
    "        # Combine the kernels\n",
    "        kernel = sum(kernel_combo)\n",
    "\n",
    "        # Instantiate the model with the combined kernel\n",
    "        gp_model = GaussianProcessRegressor(\n",
    "            kernel=kernel, n_restarts_optimizer=10, normalize_y=False\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        gp_model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the model\n",
    "        score = gp_model.score(X_test, y_test)\n",
    "\n",
    "        # Check if this model has a higher score than the best one so far\n",
    "        if score > best_score:\n",
    "            best_model = gp_model\n",
    "            best_kernel_combo = kernel_combo\n",
    "            best_score = score\n",
    "\n",
    "    # Return the best model, its kernel combination, and other data\n",
    "    return best_model, best_kernel_combo, X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# usage:\n",
    "gp_model, best_kernel_combo, X_train, X_test, y_train, y_test = train_emulator(\n",
    "    params, var\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17ba83da-f802-4b40-a9b2-00ea64142a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a master function that can take in pair of values after renaming\n",
    "def process_parameter_data():\n",
    "    # Function to create the parameter names dictionary\n",
    "    parameter_csv_path = \"/glade/u/home/djk2120/oaat_clm5_ppe/pyth/cats.csv\"\n",
    "    def create_parameter_names_dict():\n",
    "        data = {\n",
    "            key.upper(): value for key, value in {\n",
    "                'FUN_fracfixers': 0, 'KCN': 1, 'a_fix': 2, 'crit_dayl': 3, 'd_max': 4, 'fff': 5,\n",
    "                'froot_leaf': 6, 'fstor2tran': 7, 'grperc': 8, 'jmaxb0': 9, 'jmaxb1': 10, 'kcha': 11,\n",
    "                'kmax': 12, 'krmax': 13, 'leaf_long': 14, 'leafcn': 15, 'lmr_intercept_atkin': 16,\n",
    "                'lmrha': 17, 'lmrhd': 18, 'medlynintercept': 19, 'medlynslope': 20, 'nstem': 21,\n",
    "                'psi50': 22, 'q10_mr': 23, 'slatop': 24, 'soilpsi_off': 25, 'stem_leaf': 26,\n",
    "                'sucsat_sf': 27, 'theta_cj': 28, 'tpu25ratio': 29, 'tpuse_sf': 30, 'wc2wjb0': 31\n",
    "            }.items()\n",
    "        }\n",
    "        return data\n",
    "    #this parts gets the naming for the bar so, its required, this param df is later used in gaussian regression plot\n",
    "    param_df = create_parameter_names_dict()\n",
    "\n",
    "    # Read csv file\n",
    "    full_parameter = pd.read_csv(parameter_csv_path, index_col=False)\n",
    "    full_parameter = full_parameter[[\"param\", \"cat\", \"color\"]]\n",
    "    \n",
    "    # Capitalize the \"param\" column\n",
    "    full_parameter['param'] = full_parameter['param'].str.upper()\n",
    "    \n",
    "    # Convert parameter names dictionary to DataFrame\n",
    "    paramter_dataframe = pd.DataFrame.from_dict(create_parameter_names_dict(), orient='index', columns=['Values'])\n",
    "    \n",
    "    # Perform inner join\n",
    "    result = pd.merge(paramter_dataframe, full_parameter, left_index=True, right_on='param', how='inner')\n",
    "    \n",
    "    # Convert the resulting DataFrame to dictionary\n",
    "    result_dict = {}\n",
    "    for index, row in result.iterrows():\n",
    "        param = row['Values']\n",
    "        values = row.drop('Values').tolist()\n",
    "        result_dict[param] = values\n",
    "    \n",
    "    return result_dict, param_df\n",
    "\n",
    "# Call the function with the CSV file path\n",
    "result_dict, param_df = process_parameter_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299379ac-5eae-4584-8c66-70b5f53b73db",
   "metadata": {},
   "source": [
    "Build a master functio ntaht integrates uncertainty analyis with fourier test, and produces visualizations for any sets of values of parameter and variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "462ba9db-1f1f-40f9-ae22-11f3e08fa0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new added that has global ylim values\n",
    "def gaussian_regression_line(model):\n",
    "   #get the index for parameters\n",
    "    index = param_df.get(param_name)\n",
    "    \n",
    "    # Initialize variables to store min and max y values globally\n",
    "    min_y_value_global = float('inf')\n",
    "    max_y_value_global = float('-inf')\n",
    "    \n",
    "    for param_index in range(32):\n",
    "        # Generate x_values with 32 dimensions\n",
    "        x_values = np.full((10, 32), 0.5)  # Fill array with 0.5\n",
    "        x_values[:, param_index] = np.linspace(0, 5, 10)  # Set the current parameter values to evenly spaced values from 0 to 1\n",
    "\n",
    "        # Predict mean and standard deviation of the Gaussian process at each point in x_values\n",
    "        y_mean, y_std = model.predict(x_values, return_std=True)\n",
    "        \n",
    "        # Calculate the z-score for the 99.7% confidence interval\n",
    "        z_score = norm.ppf(0.99865)  # 99.7th percentile (three standard deviations)\n",
    "\n",
    "        # Calculate y values for the 99.7% confidence interval\n",
    "        y_lower = y_mean - z_score * y_std\n",
    "        y_upper = y_mean + z_score * y_std\n",
    "        \n",
    "        # Update global min and max y values\n",
    "        min_y_value_global = min(min_y_value_global, np.min(y_lower))\n",
    "        max_y_value_global = max(max_y_value_global, np.max(y_upper))\n",
    "        \n",
    "    # Generate x_values with 32 dimensions\n",
    "    x_values = np.full((10, 32), 0.5)  # Fill array with 0.5\n",
    "    x_values[:, index] = np.linspace(0, 1, 10)\n",
    "\n",
    "    # Predict mean and standard deviation of the Gaussian process at each point in x_values\n",
    "    y_mean, y_cov = model.predict(x_values, return_cov=True)\n",
    "\n",
    "    # Plot the mean line\n",
    "    plt.plot(x_values[:, index], y_mean, color='blue', linestyle='-', label='Gaussian Regression Line')\n",
    "\n",
    "    # Calculate the z-score for the 99.7% confidence interval\n",
    "    z_score = norm.ppf(0.99865)  # 99.7th percentile (three standard deviations)\n",
    "\n",
    "    # Plot the shaded region for the 99.7% confidence interval with three standard deviations\n",
    "    plt.fill_between(x_values[:, index], y_mean - 1.96 * np.sqrt(np.diag(y_cov)), y_mean + 1.96 *\n",
    "                 np.sqrt(np.diag(y_cov)), alpha=0.2, color='k', label='95% Confidence Interval')\n",
    "\n",
    "    plt.scatter(x_values[:, index], y_mean, c='r', label='Training Data')\n",
    "\n",
    "\n",
    "    # Plot the z-score value on the plot\n",
    "    plt.text(0.5, 0.5, f'99.7% CI (z-score: {z_score:.2f})', transform=plt.gca().transAxes, fontsize=10, verticalalignment='bottom')\n",
    "    plt.xlabel(f'{param_name} values between 0 and 1')\n",
    "    plt.ylabel(f'{param_name} set to 1, rest all parameters to 0.5')\n",
    "    plt.title('Actual Training Data vs Gaussian Regression Line with 99.7% Confidence Interval')\n",
    "    plt.ylim(min_y_value_global, max_y_value_global)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d44a54-76df-4983-ab2c-55159aea93e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "gaussian_regression_line(gp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aae0be9c-0048-411c-a4a4-c2989807dca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the horizontal bar graph with first order sensitivity index\n",
    "def fast_sensitivity_and_inset(model):\n",
    "    # Define a custom function to generate the Gaussian regression line for each parameter\n",
    "    def gaussian_regression_lines(model):\n",
    "        fourier_amplitudes = []  # List to store Fourier amplitudes for each parameter\n",
    "        category_colors = {}  # Dictionary to store colors for each category\n",
    "\n",
    "        for param_index in range(32):\n",
    "            # Generate x_values with 32 dimensions\n",
    "            x_values = np.full((10, 32), 0.5)  # Fill array with 0.5\n",
    "            x_values[:, param_index] = np.linspace(0, 1, 10)  # Set the current parameter values to evenly spaced values from 0 to 1\n",
    "\n",
    "            # Predict mean and standard deviation of the Gaussian process at each point in x_values\n",
    "            y_mean, _ = model.predict(x_values, return_std=True)\n",
    "\n",
    "            # Compute Fourier transform of the model output\n",
    "            y_fft = fft(y_mean)\n",
    "\n",
    "            # Compute amplitude of each frequency component\n",
    "            amplitude = np.abs(y_fft)\n",
    "\n",
    "            # Store the amplitude corresponding to the first non-zero frequency (excluding DC component)\n",
    "            fourier_amplitudes.append(amplitude[1])\n",
    "\n",
    "            # Extract category and color from param_dict\n",
    "            category = result_dict[param_index][1]\n",
    "            color = result_dict[param_index][2]\n",
    "\n",
    "            # If category not in category_colors, add it with its color\n",
    "            if category not in category_colors:\n",
    "                category_colors[category] = color\n",
    "\n",
    "        return fourier_amplitudes, category_colors\n",
    "\n",
    "    # Calculate Fourier amplitudes and category colors\n",
    "    fourier_amplitudes, category_colors = gaussian_regression_lines(model)\n",
    "\n",
    "    # Sort parameters based on Fourier amplitudes in descending order\n",
    "    sorted_indices = np.argsort(fourier_amplitudes)\n",
    "    sorted_fourier_amplitudes = np.array(fourier_amplitudes)[sorted_indices]\n",
    "\n",
    "    # Extract parameter names corresponding to sorted indices from lookup table\n",
    "    sorted_parameter_names = [result_dict[index][0] for index in sorted_indices]\n",
    "\n",
    "    # Plot horizontal bar chart\n",
    "    fig, ax = plt.subplots(figsize=(12, 14))\n",
    "    bars = ax.barh(range(len(sorted_fourier_amplitudes)), sorted_fourier_amplitudes)\n",
    "\n",
    "    # Add legend based on categories and colors\n",
    "    legend_patches = []\n",
    "    for category, color in category_colors.items():\n",
    "        legend_patches.append(mpatches.Patch(color=color, label=category))\n",
    "        fig.legend(handles=legend_patches, bbox_to_anchor=[0.5, 0.15, 0.36, 0.5], bbox_transform=fig.transFigure, fontsize='x-large')\n",
    "\n",
    "    # Set colors of bars based on categories\n",
    "    for bar, index in zip(bars, sorted_indices):\n",
    "        category = result_dict[index][1]\n",
    "        bar.set_color(category_colors[category])\n",
    "\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xlabel('Fourier Amplitude')\n",
    "    ax.set_title(f'Fourier amplitude sensitivity test (FAST) for {var_name}')\n",
    "    ax.set_yticks(range(len(sorted_fourier_amplitudes)))\n",
    "    ax.set_yticklabels(sorted_parameter_names)\n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    #ax.set_aspect('auto', adjustable='box')\n",
    "\n",
    "    # Define cross_val_plot function\n",
    "    def cross_val_plot(model, X_train, y_train, X_test, y_test, ax=None):\n",
    "        # Perform 5-fold cross-validation predictions\n",
    "        y_pred_cv = cross_val_predict(model, X_train, y_train, cv=5)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate R-squared\n",
    "        r_squared = r2_score(y_test, y_pred)\n",
    "\n",
    "        # Calculate standard error\n",
    "        std_error = np.std(y_test - y_pred)\n",
    "\n",
    "        # Plotting\n",
    "        if ax is None:\n",
    "            plt.figure(figsize=(8, 8))\n",
    "        else:\n",
    "            ax.scatter(y_test, y_pred, edgecolors=(0, 0, 0))\n",
    "            ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "            ax.errorbar(y_test, y_pred, yerr=std_error, fmt='none', ecolor='black', alpha=0.5)\n",
    "            if r_squared < 0.7:\n",
    "                ax.annotate(f'⚠ Warning: Cross-Validation R-squared: {r_squared:.2f} < 0.7.', xy=(0.5, 0.9), xycoords='axes fraction', ha='center', fontsize=8, color='red')\n",
    "            else:\n",
    "                ax.annotate(f'Cross-Validation R-squared: {r_squared:.2f}', xy=(0.5, 0.9), xycoords='axes fraction', ha='center', fontsize=10, color='black')\n",
    "\n",
    "    # Call the nested cross_val_plot function\n",
    "    cross_val_plot(model, X_train, y_train, X_test, y_test, ax=fig.add_axes([0.55, 0.25, 0.30, 0.25]))  # Inset plot\n",
    "\n",
    "    # Save the plot before displaying it\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400cdcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the entire model on the dataset\n",
    "fast_sensitivity_and_inset(gp_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
